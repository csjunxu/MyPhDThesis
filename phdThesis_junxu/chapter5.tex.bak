\chapter{Fast Tracking via Spatio-Temporal Context Learning}\label{chap:STC}

In the afore-mentioned chapters, the sophisticated Haar-like features are used via feature selection or extraction techniques to build efficient and effective appearance models. However, these methods do not fully take advantage of the spatio-temporal context information, which is helpful to determine target location. In this chapter, we present a simple yet fast and robust algorithm which exploits the spatio-temporal context for visual tracking.
%
\section{Introduction}
%
%
Generally speaking, both generative and discriminative tracking
algorithms face trade-off between effectiveness and efficiency of
an appearance model.
%
Notwithstanding much progress has been made in recent years, it
remains a challenging task to develop an efficient and robust tracking
algorithm.

%
\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.8\linewidth]{ffigs/flowa}\\
\scriptsize
(a) Learn spatial context at the $t$-th frame\\
\includegraphics[width=1\linewidth]{ffigs/flowb}\\
\scriptsize
(b) Detect object location at the $(t\rm{+}1)$-th frame\\
\end{center}
\caption{Basic flow of our tracking algorithm. The local context
  regions are inside the red rectangles while the target locations are
  indicated by the yellow rectangles. FFT denotes the Fast Fourier Transform
  and IFFT is the inverse FFT.}
\label{fig:basicflow}
\end{figure*}
%
%
In visual tracking, a local context consists of a target object and
its immediate surrounding background within a determined region
(See the regions inside the red rectangles in Figure~\ref{fig:basicflow}).
%
Therefore, there exists a strong spatio-temporal relationship between
the local scenes surrounding the object in consecutive frames.
%
For instance, the target of the~{\textit{bolt}} sequence in
Figure~\ref{fig:occlusion} undergoes non-rigid
deformation which makes the object appearance change significantly.
%
However, the local context surrounding the object does not change much
as the overall appearance remains similar.
%
Thus, the presence of local background in the current frame helps
predict the object location in the next frame.
%
This temporally proximal information in consecutive frames is
the temporal context which has been recently applied to object
detection~\cite{divvala2009empirical}.
%
Moreover, the spatial relation between object and its local background
provides specific information about the configuration of a scene.
%
For example, the tracker is easily distracted from one to
another when two nearby deers jump together
(See the {\textit{animal}} sequence in
Figure~\ref{fig:animal}) if only appearance
information is used.
%
However, the spatial context can help reduce
this ambiguity because the configurations of the local scenes
surrounding both objects are different (e.g., $\#53$ in the
{\textit{animal}} sequence in Figure~\ref{fig:animal}).
%
Although there exist similar deers in the local backgrounds,
their spatial contexts are different: the distractor is at the bottom
left side of the scene surrounding the currently tracked object while the
currently tracked object is at the top right side of the scene
surrounding the distractor.
%
Recently, several methods~\cite{yang2009context, grabner2010tracking,
  dinh2011context, Wen_ECCV_2012}
exploit context information to facilitate visual tracking with
demonstrated success.
%
However, these approaches require high computational loads for feature
extraction in training and detection phases.

%
Figure~\ref{fig:basicflow} illustrates the basic flow of
our algorithm.
%
First, we learn a spatial context model between
the target object and its local surrounding background based on their
spatial correlations in a scene which is formulated as a deconvolution
problem.
%
Next, the learned spatial context model is used to
update a spatio-temporal context model for the next frame.
%
Then, tracking in the next frame is formulated by computing a
confidence map as a convolution problem that integrates the
spatio-temporal context information, and the best object
location can be estimated by maximizing the confidence map (See
Figure~\ref{fig:basicflow} (b)).
%
Experiments on various challenging
sequences demonstrate the proposed algorithm performs favorably
against state-of-the-art methods in terms of accuracy, efficiency and
robustness.
%
%=======================================================================
\section{Problem Formulation}
 Our tracking task is formulated by computing a confidence map which
 estimates the object location likelihood:
%-------------------------------------
\begin{equation}
c(\textbf{x})=P(\textbf{x}|o),
 \label{eq:confmap}
\end{equation}
%-------------------------------------------------------------------------
where $\textbf{x}\in\mathbb{R}^2$ is an object location and $o$ denotes
the object present in the scene.
%
In the following, the spatial context information is used to estimate
(\ref{eq:confmap}) where
Figure~\ref{fig:graphrepresentation} shows its graphical model
representation.

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.5\linewidth]{ffigs/graphrepresentation}
\end{center}
\caption{Graphical model representation of spatial relationship
between object and its local context.
%
The local context region $\Omega_c$ is inside the red rectangle which
includes object region $\Omega_o$ surrounding by the yellow rectangle
centering at the tracked result $\textbf{x}^\star$.
%
The context feature at location $\textbf{z}$ is denoted by
 $\textbf{c}(\textbf{z})=(I(\textbf{z}),\textbf{z})$ including
a low-level appearance representation (i.e., image intensity
$I(\textbf{z})$) and location information.}
\label{fig:graphrepresentation}
\end{figure}
%
In the current frame, we have the object location $\textbf{x}^{\star}$
(i.e., coordinate of the tracked object center).
%
Then, the context feature set is defined as
$X^c=\{\textbf{c}(\textbf{z})=(I(\textbf{z}),\textbf{z})|
\textbf{z}\in\Omega_c(\textbf{x}^\star)\}$
where $I(\textbf{z})$ denotes image intensity at location $\textbf{z}$
and $\Omega_c(\textbf{x}^\star)$ is the  neighborhood  of location
$\textbf{x}^\star$ with size two times of the object size.
%
By marginalizing the joint probability
$P(\textbf{x},\textbf{c}(\textbf{z})|o)$, the object location likelihood
function in (\ref{eq:confmap}) can be formulated as
%-------------------------------------
\begin{equation}
\begin{aligned}
c(\textbf{x})&=P(\textbf{x}|o)\\
&=\textstyle\sum_{\textbf{c}(\textbf{z})\in X^c}P(\textbf{x},\textbf{c}(\textbf{z})|o)
\\
&=\textstyle\sum_{\textbf{c}(\textbf{z})\in
  X^c}P(\textbf{x}|\textbf{c}(\textbf{z}),o)P(\textbf{c}(\textbf{z})|o)
,
\end{aligned}
 \label{eq:marginalizingconfmap}
\end{equation}
%-------------------------------------------------------------------------
where the conditional probability
$P(\textbf{x}|\textbf{c}(\textbf{z}),o)$ models the spatial
relationship between the object location and its context information which
helps resolve ambiguities when the image measurements allow
different interpretations, and
$P(\textbf{c}(\textbf{z})|o)$ is a context prior probability which
models appearance of the local context.
%
The main task in this work is to learn
$P(\textbf{x}|\textbf{c}(\textbf{z}),o)$ because it bridges the gap
between object location and its spatial context.
%
\subsection{Spatial Context Model}
The conditional probability function
$P(\textbf{x}|\textbf{c}(\textbf{z}),o)$ in
(\ref{eq:marginalizingconfmap}) is defined as
\begin{equation}
P(\textbf{x}|\textbf{c}(\textbf{z}),o)=h^{sc}(\textbf{x}-\textbf{z}),
 \label{eq:spatialcontext}
\end{equation}
%
where $h^{sc}(\textbf{x}-\textbf{z})$ is a function
(See Section~\ref{sec:fast-learning}) with respect to
the relative {\textit{distance}} and {\textit{direction}} between object
location $\textbf{x}$ and its local context location $\textbf{z}$,
thereby encoding the spatial relationship between an object and its
spatial context.

Note that  $h^{sc}(\textbf{x}-\textbf{z})$ is not a radially
symmetric function which means $h^{sc}(\textbf{x}-\textbf{z})\neq
h^{sc}(|\textbf{x}-\textbf{z}|)$, and thus takes into account
different spatial relationships between an object and its local contexts,
thereby helping resolve ambiguities when similar objects appear in close
proximity.
%
For example, when a method tracks an eye based only on appearance
(denoted by $\textbf{z}_l$) in the $davidindoor$ sequence shown in
Figure~\ref{fig:graphrepresentation}, the tracker may be easily
distracted to the right one (denoted by $\textbf{z}_r$)
because both of the two eyes and their surrounding backgrounds have
very similar appearances (when the object moves fast and the search
region is large).
%
However, in the proposed method, while the locations of both eyes are
at similar distances to location $\textbf{x}^\star$
(Here, it is location
of the context relative to object location $\textbf{z}_l$), their
relative locations to $\textbf{x}^\star$ are different,
thereby resulting in different spatial relationships,
%
i.e.,
$h^{sc}(\textbf{z}_{l}-\textbf{x}^\star)\neq
h^{sc}(\textbf{z}_{r}-\textbf{x}^\star)$.
%
That is, the non-radially symmetric function $h^{sc}$,
encodes different relative
location information, which helps resolve ambiguities effectively.

\subsection{Context Prior Model}
In (\ref{eq:marginalizingconfmap}), the context prior probability is
simply modeled as
%
\begin{equation}
P(\textbf{c}(\textbf{z})|o)=I(\textbf{z})w_{\sigma}(\textbf{z}-\textbf{x}^\star),
 \label{eq:context-prior}
\end{equation}
where $I(\cdot)$ is image intensity which is the feature to represent appearance of
context. This feature is very simple which can achieve good results by considering context information.$w_{\sigma}(\cdot)$ is a weighted function defined as
\begin{equation}
w_{\sigma}(\textbf{z})=ae^{-\frac{|\textbf{z}|^2}{\sigma^2}},
 \label{eq:weight}
\end{equation}
%
where $a$ is a normalization constant and $\sigma$ is a scale parameter.

%
In (\ref{eq:context-prior}), it models focus of attention that is
motivated by the biological visual system which
concentrates on certain image regions requiring detailed
analysis~\cite{torralba2003contextual}.
%
The closer the context
location $\textbf{z}$ is to the currently tracked target location
$\textbf{x}^\star$, the more important it is to predict the object
location in the coming frame, and larger weight should be
set.
%
Different from our algorithm that uses a spatially weighted function to
indicate the importance of context at different locations,
there exist other methods~\cite{belongie2002shape,wolf2006critical} in
which spatial sampling techniques are used to focus more detailed contexts
at the locations near the object center (i.e., the closer the location is
to the object center, the more context locations are sampled).

\subsection{Confidence Map}
%
The confidence map of an object location is modeled as
%
\begin{equation}
c(\textbf{x})=P(\textbf{x}|o)=be^{-|\frac{\textbf{x}-\textbf{x}^\star}{\alpha}|^\beta},
 \label{eq:confidencemap}
\end{equation}
%
where $b$ is a normalization constant, $\alpha$ is a scale parameter
and $\beta$ is a shape parameter (See
Figure~\ref{fig:confidencemap}). This confidence map function does not take into account the motion prediction and our motion model is simply such at the tracking location at the $t$-th frame is equally likely to appear at a context region surrounding the old tracked location at the $(t-1)$-th frame

%
\begin{figure}[t]
\begin{center}
\includegraphics[width=0.5\linewidth]{ffigs/confb}\\
\end{center}
   \caption{Illustration of 1-D cross section of the confidence map $c$(\textbf{x}) in (\ref{eq:confidencemap}) with different parameters $\beta$. Here, the object location $\textbf{x}^\star=(100,100)$.}
\label{fig:confidencemap}
\end{figure}
%
The object location ambiguity problem often occurs in visual tracking
which adversely affects tracking performance.
%
In~\cite{Babenko_pami_2011},
a multiple instance learning technique is adopted to handle location
ambiguity problem with favorable tracking results.
%
%MH2: add more explanations
%KH2: It is ok.
The closer the location is to the currently tracked position, the
larger probability that the ambiguity occurs with (e.g., predicted
object locations that differ by a few pixels are all plausible
solutions and thereby cause ambiguities).
%
In our method, we resolve the location ambiguity problem by choosing a
proper shape parameter $\beta$.
%
As illustrated in Figure~\ref{fig:confidencemap}, a large $\beta$
(e.g., $\beta=2$) results in an oversmoothing effect for the
confidence value at locations near to the object center, thereby
failing to effectively reduce location ambiguities.
%
On the other hand, a small $\beta$ (e.g.,
$\beta=0.5$) yields a sharp peak near the object center,
thereby only activating much fewer positions when learning the spatial
context model.
%
This in turn may lead to overfitting in searching for the object
location in the coming frame.
%
We find that robust results can be obtained when $\beta=1$ in our
experiments.

%
\subsection{Fast Learning Spatial Context Model}
\label{sec:fast-learning}
We have presented the formulations of the confidence
map function (\ref{eq:confidencemap}) and the context prior model
(\ref{eq:context-prior}), and our objective is to learn the spatial
context model (\ref{eq:spatialcontext}).
%
To this end, putting (\ref{eq:confidencemap}),
(\ref{eq:context-prior}) and (\ref{eq:spatialcontext}), we formulate
(\ref{eq:marginalizingconfmap}) as
%
\begin{equation}
\begin{aligned}
c(\textbf{x})&=be^{-|\frac{\textbf{x}-\textbf{x}^\star}{\alpha}|^\beta}\\
&=\textstyle\sum_{\textbf{z}\in \Omega_c(\textbf{x}^\star)}
h^{sc}(\textbf{x}-\textbf{z})
I(\textbf{z})w_{\sigma}(\textbf{z}-\textbf{x}^\star)\\
&=h^{sc}(\textbf{x})\otimes(I(\textbf{x})
w_{\sigma}(\textbf{x}-\textbf{x}^\star)),
\end{aligned}
 \label{eq:convolutionconfmap}
\end{equation}
%
where $\otimes$ denotes the convolution operator.

We note (\ref{eq:convolutionconfmap}) can be transformed to the
Fourier domain in which the Fast Fourier Transform (FFT)
algorithm~\cite{oppenheim1983signals} can be used for fast
convolution. That is,
%
\begin{equation}
\mathcal{F}(be^{-|\frac{\textbf{x}-\textbf{x}^\star}{\alpha}|^\beta})=
\mathcal{F}(h^{sc}(\textbf{x}))\odot
\mathcal{F}(I(\textbf{x})w_{\sigma}(\textbf{x}-\textbf{x}^\star)),
\label{eq:fft}
\end{equation}
%
where $\mathcal{F}$ denotes the FFT function and $\odot$ is
the element-wise product. Therefore, we have
%
\begin{equation}
h^{sc}(\textbf{x}) = \mathcal{F}^{-1}\bigg(
%\begin{pmatrix}
\frac{\mathcal{F}(be^{-|\frac{\textbf{x}-\textbf{x}^\star}{\alpha}|^\beta})}
{\mathcal{F}(I(\textbf{x})w_{\sigma}(\textbf{x}-\textbf{x}^\star))}
\bigg),
%\end{pmatrix}
\label{eq:fftspatialcontextmodel}
\end{equation}
%
where $\mathcal{F}^{-1}$ denotes the inverse FFT function.

%=========================================================================
\section{Proposed Tracking Algorithm}
In this section, we present our algorithm in
details.
%
Figure~\ref{fig:basicflow} shows the basic flow of our
algorithm.
%
The tracking problem is formulated as a detection task.
%
We assume that the target location in the first frame has been
initialized manually or by some object detection algorithms.
%
At the $t$-th frame, we learn the spatial context model
$h_{t}^{sc}(\textbf{x})$ (\ref{eq:fftspatialcontextmodel}),
which is used to update the spatio-temporal context model
$H_{t+1}^{stc}(\textbf{x})$ (\ref{eq:spatiotemporal}) that is applied
to detect the object location in the $(t\rm{+}1)$-th frame.
%
When the $(t\rm{+}1)$-th frame arrives, we crop out the local context region
$\Omega_{c}(\textbf{x}_{t}^\star)$ based on the tracked location
$\textbf{x}_{t}^\star$ at the $t$-th frame and construct the
corresponding context feature set
$X_{t+1}^c=\{\textbf{c}(\textbf{z})=
(I_{t+1}(\textbf{z}),\textbf{z})|\textbf{z}\in\Omega_c(\textbf{x}_t^\star)\}$.
%
Finally, the object location $\textbf{x}_{t+1}^\star$ in the
$(t\rm{+}1)$-th frame is determined by maximizing the new confidence map
%
\begin{equation}
\textbf{x}_{t+1}^\star=\mathop{\arg\max}_{\textbf{x}\in
 \Omega_c(\textbf{x}_t^\star)}c_{t+1}(\textbf{x}),
\label{eq:confmaptt}
\end{equation}
%
where $c_{t+1}(\textbf{x})$ is represented as
\begin{equation}
c_{t+1}(\textbf{x})=\mathcal{F}^{-1}\bigg(\mathcal{F}(H_{t+1}^{stc}(\textbf{x}))
\odot
\mathcal{F}(I_{t+1}(\textbf{x})w_{\sigma}(\textbf{x}-\textbf{x}_t^\star))\bigg).
\label{eq:confmapdeftt}
\end{equation}
%
We don't directly estimate $c_{t+1}$ from $c_t$ because the target appearance may change much and we need the online update spatio-temporal context model to adapt these appearance changes.
%
%
\begin{figure}[t]
\begin{center}
\includegraphics[width=0.56\linewidth]{ffigs/filter}
%\includegraphics[width=0.4\linewidth]{figs/hstcfreq}
\end{center}
   \caption{Illustration of the amplitude of
     the temporal filter $F_\omega$ in (\ref{eq:filter}).}
     %and the spatio-temporal context model
     %$H_{t}^{stc}$ with respect to its spatial domain; Left: temporal
     %filter (\ref{eq:filter}); Right: spatio-temporal context
     %model $H_t^{stc}$ (\ref{eq:temporalFrequency}) (for the $340$-th
     %frame from the {\textit{davidindoor}} sequence), and for better
     %visualization, the zero-frequency component is moved to the
     %middle of the spectrum.}
\label{fig:lowpassfilter}
\end{figure}
%
\subsection{Update of Spatio-Temporal Context}
%
The spatio-temporal context model is updated by
%
\begin{equation}
H_{t+1}^{stc}=(1-\rho)H_{t}^{stc}+\rho h_t^{sc},
\label{eq:spatiotemporal}
\end{equation}
where $\rho$ is a learning parameter and $h_t^{sc}$ is the spatial
context model computed by (\ref{eq:fftspatialcontextmodel}) at
the $t$-th frame.
%
We note (\ref{eq:spatiotemporal}) is a temporal filtering procedure
which can be easily observed in frequency domain
%
\begin{equation}
H_{\omega}^{stc} = F_{\omega}h_{\omega}^{sc},
\label{eq:temporalFrequency}
\end{equation}
where $H_{\omega}^{stc}\triangleq\int H_t^{stc}e^{-j\omega t}dt$ is
the temporal Fourier transform of $H_{t}^{stc}$ and similar to
$h_{\omega}^{sc}$. The temporal filter $F_{\omega}$ is formulated as
%
\begin{equation}
F_{\omega}=\frac{\rho}{e^{j\omega}-(1-\rho)},
\label{eq:filter}
\end{equation}
%
where $j$ denotes imaginary unit.
%
It is easy to validate that
$F_{\omega}$ in (\ref{eq:filter}) is a low-pass
filter~\cite{oppenheim1983signals} which is illustrated in
Figure~\ref{fig:lowpassfilter} that shows its profile in one
period.
%
%KH2: The following sentence refers to the right figure of Figure5 that I have eliminated
\ignore{
Moreover, from the spatial frequency spectrum of
$H_{t}^{stc}$ shown on the right hand side of
Figure~\ref{fig:lowpassfilter}, we observe that most of its energy
concentrates on the low-frequency domain.}
%
Therefore, our
spatio-temporal context model is able to effectively filter out
image noise introduced by appearance variations, thereby leading to
more stable results.
%
%
\begin{figure}[t]
\begin{center}
\includegraphics[width=0.6\linewidth]{ffigs/figocc}
\end{center}
   \caption{Illustration of why the proposed model is equipped to
handle heavy occlusion.
%
The object, background, and occlusion regions are
denoted by $\Omega_o$, $\Omega_b$ and $\Omega_{occ}$,
respectively.
%
The context region $\Omega_c=\Omega_o\bigcup\Omega_b$. See text for details.}
\label{fig:occ}
\end{figure}
%
\subsection{Discussion}
\label{discussion}
We note that the low computational complexity is one prime
characteristic of the proposed algorithm in which only $6$ FFT
operations are involved when processing one frame including learning
the spatial context model (\ref{eq:fftspatialcontextmodel}) and
computing the confidence map (\ref{eq:confmapdeftt}).
%
The computational complexity for
computing each FFT is only $\mathcal{O}(MN\log(MN))$ for the local
context region of $M\times N$ pixels, thereby resulting in a very fast
method ($350$ frames per second in MATLAB on an i$7$ machine).
%
Furthermore, the proposed
algorithm achieves robust results as discussed bellow.
%
{\flushleft\textbf{Difference with related work.}}
It should be noted
that the proposed spatio-temporal context tracking algorithm is
significantly different from the recently proposed context based
methods~\cite{yang2009context,grabner2010tracking, dinh2011context,
  Wen_ECCV_2012} and the approaches that use FFT for efficient
computation~\cite{bolme2009average, bolme2010visual,
  henriques2012circulant}.

All the above-mentioned context-based methods adopt some
strategies to find regions with consistent motion correlations to the
object.
%
In~\cite{yang2009context}, a data mining method is used to
extract segmented regions surrounding the object as auxiliary objects
for collaborative tracking.
%
To find consistent regions, key points surrounding the
object are first extracted to help locate the object
position~\cite{grabner2010tracking,dinh2011context,Wen_ECCV_2012}.
%
Next, SIFT or SURF descriptors are used to represent
these consistent regions~\cite{grabner2010tracking, dinh2011context,Wen_ECCV_2012}.
%
Thus, computationally expensive operations are required in
representing and finding consistent regions.
%
%MH2: change can to useful... it is odd to have .... can.... may....
%KH2: OK. Thanks!
Moreover, due to the sparsity nature of key points, some
consistent regions that are useful for locating the object position
may be discarded.
%
However, the proposed algorithm does not have these problems
because all the local regions surrounding the object are
considered as the potentially consistent regions, and the motion
correlations between the objects and its local contexts in consecutive
frames are learned by the spatio-temporal
context model that is efficiently computed by FFT.

In~\cite{bolme2009average,bolme2010visual}, the formulations are based
on correlation filters that are directly obtained by classic signal
processing algorithms.
%
At each frame, correlation filters are trained using a large
number of samples, and then combined to find the most correlated
position in the next frame.
%
In~\cite{henriques2012circulant}, the filters
proposed by~\cite{bolme2009average,bolme2010visual} are
kernelized and used to achieve more stable results.
%
The proposed algorithm is significantly different
from~\cite{bolme2009average, bolme2010visual, henriques2012circulant}
in several aspects.
%
First, our algorithm models the spatio-temporal relationships between
the object and its local contexts which is motivated by the human
visual system that uses context to help resolve ambiguities in complex
scenes efficiently and effectively.
%
Second, our algorithm focuses on the regions which require
detailed analysis, thereby effectively reducing the adverse effects of
background clutters and leading to more robust results.
%
%MH2: proper shape parameter -> poroper prior distribution
%KH2: OK.
Finally, our algorithm handles the object location ambiguity problem
using the confidence map with a proper prior distribution,
thereby achieving more stable and accurate performance for visual
tracking.
%
{\flushleft\textbf{Robustness to occlusion.}}
As shown by the example in Figure~\ref{fig:occ}, it can be rationally
assumed that image intensity $I_{t+1}(\textbf{z})\thickapprox
I_{t}(\textbf{z})$ for $\textbf{z}\in \Omega_c\backslash\Omega_{occ}$ due to
the temporal continuity between frames.
%
Then, we formulate the confidence map (\ref{eq:confmapdeftt}) at the
$(t\rm{+}1)$-th frame as
%
\begin{equation}
\begin{aligned}
c_{t+1}(\textbf{x})&=H_{t+1}^{stc}(\textbf{x})\otimes(I_{t+1}(\textbf{x})w_{\sigma}(\textbf{x}-\textbf{x}_t^\star))\\
%&=\textstyle\sum_{\textbf{z}\in\Omega_c}H_{t+1}^{stc}(\textbf{x}-\textbf{z})(I_{t+1}(\textbf{z})w_{\sigma}(\textbf{z}-\textbf{x}_t^\star))\\
&=\textstyle\sum_{\textbf{z}\in\Omega_{occ}}H_{t+1}^{stc}(\textbf{x}-\textbf{z})(I_{t+1}(\textbf{z})w_{\sigma}(\textbf{z}-\textbf{x}_t^\star))\\
&+\textstyle\sum_{\textbf{z}\in\Omega_c\backslash\Omega_{occ}}H_{t+1}^{stc}(\textbf{x}-\textbf{z})(I_{t+1}(\textbf{z})w_{\sigma}(\textbf{z}-\textbf{x}_t^\star))\\
&\approx \textstyle\sum_{\textbf{z}\in\Omega_{occ}}H_{t+1}^{stc}(\textbf{x}-\textbf{z})(I_{t+1}(\textbf{z})w_{\sigma}(\textbf{z}-\textbf{x}_t^\star))\\
&+\textstyle\sum_{\textbf{z}\in\Omega_c\backslash\Omega_{occ}}H_{t+1}^{stc}(\textbf{x}-\textbf{z})(I_{t}(\textbf{z})w_{\sigma}(\textbf{z}-\textbf{x}_t^\star))\\
%&=\textstyle\sum_{\textbf{z}\in\Omega_{occ}}H_{t+1}^{stc}(\textbf{x}-\textbf{z})(\Delta %I_{t+1}(\textbf{z})w_{\sigma}(\textbf{z}-\textbf{x}_t^\star))\\
%&+\textstyle\sum_{\textbf{z}\in\Omega_c}H_{t+1}^{stc}(\textbf{x}-\textbf{z})(I_{t}(\textbf{z})w_{\sigma}(\textbf{z}-\textbf{x}_t^\star))\\
&=c_{t+1}^{occ}(\textbf{x})+\widehat{c}_{t}(\textbf{x}),
\end{aligned}
\label{eq:confmapttocc}
\end{equation}
%
where $c_{t+1}^{occ}(\textbf{x})$ and $\widehat{c}_{t}(\textbf{x})$ are denoted by
%
\begin{equation}
c_{t+1}^{occ}(\textbf{x})=
\textstyle\sum_{\textbf{z}\in\Omega_{occ}}H_{t+1}^{stc}(\textbf{x}-\textbf{z})
(\Delta I_{t+1}(\textbf{z})w_{\sigma}(\textbf{z}-\textbf{x}_t^\star)),\nonumber
\end{equation}
\begin{equation}
\widehat{c}_{t}(\textbf{x})=
\textstyle\sum_{\textbf{z}\in\Omega_c}
H_{t+1}^{stc}(\textbf{x}-\textbf{z})
(I_{t}(\textbf{z})w_{\sigma}(\textbf{z}-\textbf{x}_t^\star)),
\label{eq:contextocc}
\end{equation}
%
where $\Delta I_{t+1}=I_{t+1}-I_t$.

Using (\ref{eq:spatiotemporal}), (\ref{eq:confmapdeftt}) and (\ref{eq:convolutionconfmap}), (\ref{eq:contextocc}) can be re-written
as
%
\begin{equation}
\begin{aligned}
\widehat{c}_{t}(\textbf{x})
&=(1-\rho)H_t^{stc}(\textbf{x})\otimes(I_t(\textbf{x})w_{\sigma}(\textbf{x}-\textbf{x}_t^\star))\\
&+\rho h_t^{sc}(\textbf{x})\otimes(I_t(\textbf{x})w_{\sigma}(\textbf{x}-\textbf{x}_t^\star))\\
&=(1-\rho)c_{t}(\textbf{x})+\rho be^{-|\frac{\textbf{x}-\textbf{x}_t^\star}{\alpha}|^\beta}.
\end{aligned}
\end{equation}

From Figure~\ref{fig:occ}, even if the object is fully occluded, the
context region $\Omega_c>\Omega_{occ}\approx\Omega_o$
($\Omega_c=4\Omega_o$ in our experiments), which means the confidence
map $c_{t+1}(\textbf{x})$ is determined by
$\widehat{c}_t(\textbf{x})$. Moreover, the term $\Delta I_{t+1}$ in
$c_{t+1}^{occ}(\textbf{x})$ is a
signal with high frequency
that represents drastic appearance variation introduced by occlusion
(which can also be applied to other factors such as illumination change,
non-rigid deformation, etc, as shown in
Figure~\ref{fig:challengingEnvironments}).
%
Since the filter
$H_{t+1}^{stc}(\textbf{x})$ in $c_{t+1}^{occ}(\textbf{x})$ is a
low-pass filter
%KH2: revised explanation
%
due to the low-pass filter $F_\omega$ in (\ref{eq:temporalFrequency})
(See Figure~\ref{fig:lowpassfilter}), we have
$c_{t+1}^{occ}(\textbf{x})\thickapprox 0$. Therefore, we have
%
\begin{equation}
c_{t+1}(\textbf{x})\approx \widehat{c}_t(\textbf{x})=(1-\rho)c_t(\textbf{x})+\rho be^{-|\frac{\textbf{x}-\textbf{x}_t^\star}{\alpha}|^\beta},
\end{equation}
%
which means the confidence map $c_{t+1}(\textbf{x})$ is unrelated to
%the occlusions, thereby resulting in a robust performance in dealing
%with heavy occlusion.
occlusion.
%
Therefore, the proposed algorithm achieves robust performance when the
target object is heavily occluded.
%
\section{Experiments}

We evaluate the proposed tracking algorithm based on spatio-temporal
context (STC) algorithm using $22$ video sequences with challenging factors
including heavy occlusion, drastic illumination changes, pose and
scale variation, non-rigid deformation, background cluster and motion
blur.
%
We compare the proposed STC tracker with $18$ state-of-the-art
methods.
%
The parameters of the proposed algorithm are {\textit{fixed}} for all
the experiments.
%
For other trackers, we use either the source or binary codes provided
by the authors in which parameters of each tracker are tuned for best
results.
%
The $18$ trackers we compare with
are: scale mean-shift (SMS) tracker~\cite{collins2003mean}, fragment
tracker (Frag)~\cite{Adam_CVPR_2006}, semi-supervised
Boosting tracker (SSB)~\cite{Grabner_ECCV_2008}, local orderless
tracker (LOT)~\cite{oron2012locally}, incremental visual tracking
(IVT) method~\cite{Ross_IJCV_2008}, online AdaBoost tracker
(OAB)~\cite{Grabner_BMVC_2006}, multiple instance learning tracker
(MIL)~\cite{Babenko_pami_2011}, visual tracking decomposition method
(VTD)~\cite{Kwon_CVPR_2010}, L1 tracker (L1T)~\cite{Mei_PAMI_2011},
tracking-learning-detection (TLD) method~\cite{Kalal_CVPR_2010},
distribution field tracker (DF)~\cite{sevilla2012distribution},
multi-task tracker (MTT)~\cite{zhang2012robust}, structured output
tracker (Struck)~\cite{Hare_ICCV_2011}, context tracker
(ConT)~\cite{dinh2011context}, minimum output sum of square error
(MOS) tracker~\cite{bolme2010visual}, compressive tracker
(CT)~\cite{Zhang_eccv_2012}, circulant structure tracker
(CST)~\cite{henriques2012circulant} and local-global tracker
(LGT)~\cite{cehovin2013robust}.
%
For the trackers involving randomness,
we repeat the experiments $10$ times on each sequence and
report the averaged results.
We have tested many different initial rectangles and selected the initial rectangles that yield good results for most competing methods.
%
%MH: run a fast machine to get higher frame rate
%
Implemented in MATLAB, our tracker runs
at $350$ frames per second (FPS) on an i$7$ $3.40$
GHz machine with $8$ GB RAM.
%
%
\begin{figure*}[!ht]
\begin{center}
\begin{tabular}{c}
\includegraphics[width=.27\linewidth]{ffigs/davidindoor/10}
\includegraphics[width=.27\linewidth]{ffigs/davidindoor/300}
\includegraphics[width=.27\linewidth]{ffigs/davidindoor/760}\\
\scriptsize (a) davidindoor \\
\includegraphics[width=.27\linewidth]{ffigs/davidoutdoor/84}
\includegraphics[width=.27\linewidth]{ffigs/davidoutdoor/90}
\includegraphics[width=.27\linewidth]{ffigs/davidoutdoor/188}\\
\scriptsize (b) davidoutdoor \\
\includegraphics[width=.27\linewidth]{ffigs/bolt/10}
\includegraphics[width=.27\linewidth]{ffigs/bolt/90}
\includegraphics[width=.27\linewidth]{ffigs/bolt/170}\\
\scriptsize (c) bolt \\
\includegraphics[width=.27\linewidth]{ffigs/deer/43}
\includegraphics[width=.27\linewidth]{ffigs/deer/53}
\includegraphics[width=.27\linewidth]{ffigs/deer/56}\\
\scriptsize (d) animal \\
\end{tabular}
\includegraphics[width=0.7\linewidth]{ffigs/legend}\\
\end{center}
\vspace{-0.1cm}
\caption{Tracking results of evaluated algorithms.}
\label{fig:challengingEnvironments}
\end{figure*}
%
\subsection{Experimental Setup}
The following parameters are set by experience. The size of context region is set to two times of the target size. The
parameter in the weighted function for focus of attention is set to
$\sigma=\frac{s_h+s_w}{2}$, where $s_h$ and $s_w$ are height and width
of the tracking rectangle, respectively. The parameters of the map
function are set to $\alpha=4$ and $\beta=1$. The learning parameter
$\rho=0.075$.
%
%MH2: use right term... minus ->
%KH2: OK. Thanks.
To reduce effects of illumination change,
each intensity value in the context region is normalized by
subtracting the average intensity of that region.
%
%MH2: when doing FFT.. sounds bad
Then, the intensity in the context region multiplies a Hamming
window to reduce the frequency effect of image boundary
when using FFT~\cite{oppenheim1983signals,bolme2009average}.
%
%-------------------------------------------------------------------
%
\renewcommand{\tabcolsep}{2pt}
\begin{table*}[!ht]
\caption{Success rate (SR)(\%). \textcolor{red}{\textbf{Red}} fonts
  indicate the best
  performance while the \textcolor{blue}{\textbf{blue}} fonts indicate
  the second best
  ones. The total number of evaluated frames is $9,607$.
%    (Sort in alphabetical order).
}
\label{Table1}
{%\footnotesize
\scriptsize
  %\center\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}|c|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|}\hline
%\setlength{\arraycolsep}{10pt}
\center\begin{tabular}{| c |c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\hline
Sequence          &SMS &Frag &SSB &LOT &IVT &OAB  &MIL &VTD &L1T  &TLD  &DF &MTT &Struck &ConT &MOS &CT &CST &LGT &\textbf{STC}    \\ \hline

{\textit{animal}}&13    &3	&51	&15	&4
&17	    &83 	&\textcolor{red}{\textbf{96}} 	&6	    &37	    &6	    &87	  &93	  &58	&3	   &92 &\textcolor{blue}{\textbf{94}} &7	 &\textcolor{blue}{\textbf{94}}\\
{\textit{bird}} &33      &64	&13	&5	&78	&\textcolor{red}{\textbf{94}}	&10	&9	&44	&42	 &\textcolor{red}{\textbf{94}}	 &10	 &48	&26	&11	 &8	 &47 &\textcolor{blue}{\textbf{89}}	&63\\
{\textit{bolt}}&58      &41	&18	&89	&15
&1	    &92	    &3	    &2	    &1	    &2	    &2	  &8	  &6	&25	   &\textcolor{blue}{\textbf{94}} &39	 &74 &\textcolor{red}{\textbf{96}}\\
{\textit{cliffbar}}&5 &24	&24	&26	&47
 &66	    &71  	&53	    &24	    &62	    &26	    &55	  &44	  &43	&6	   &\textcolor{blue}{\textbf{95}} &93 &81	 &\textcolor{red}{\textbf{97}}\\
{\textit{chasing}}&72   &77	&62	&20	&82
&71	    &65	    &70	    &72	    &76	    &70	    &95	  &85	  &53	&61	   &79 &\textcolor{blue}{\textbf{96}} &95	 &\textcolor{red}{\textbf{98}}\\
{\textit{car4}}&10   &34	&22	&1	&\textcolor{blue}{\textbf{97}}
 &30	    &37	    &35	    &94	    &88	    &26	    &22	  &96	  &90	&28	   &36 &44 &33	 &\textcolor{red}{\textbf{98}}\\
{\textit{car11}}&1   &1	&19	&32	&54
&14	    &48 	&25	    &46	    &67	    &\textcolor{blue}{\textbf{78}}	    &59	  &18	  &47	 &\textcolor{red}{\textbf{85}}	   &36 &48 &16	&63\\
{\textit{cokecan}}&1 &3	 &38	&4	&3
&53	    &18	    &7	    &16	    &17	    &13	    &85	  &\textcolor{red}{\textbf{94}}	  &20	&2	   &30 &86	 &18 &\textcolor{blue}{\textbf{92}}\\
{\textit{downhill}}&81   &89	 &53	&6	   &87	&82	&33	&\textcolor{red}{\textbf{98}}	&66	&13	&94	&54	 &87	 &71	 &28	&82	&72	 &73 &\textcolor{blue}{\textbf{95}}\\
{\textit{dollar}}&55  &41	&38	   &40	&21
 &16	    &46	    &39	    &39	    &39
 &\textcolor{red}{\textbf{100}}	&39
 &\textcolor{red}{\textbf{100}}	  &\textcolor{red}{\textbf{100}}
 &\textcolor{blue}{\textbf{89}}	   &87 &\textcolor{red}{\textbf{100}}
 &\textcolor{red}{\textbf{100}}	 &\textcolor{red}{\textbf{100}}\\
%MH2: what is darkcar? is it commonly used?
%KH2: I have changed it to car11
{\textit{davidindoor}}&6    &1	&36	 &20	&7
&24	    &30	    &38	    &18	    &\textcolor{blue}{\textbf{96}}	    &64	    &94	  &71	  &82	&43	   &46 &2 &95	 &\textcolor{red}{\textbf{98}}\\
{\textit{davidoutdoor}}&89   &73	    &15	&\textcolor{blue}{\textbf{96}}	&68
&15	&58	    &42	    &59	    &18	    &76	    &7	  &35	  &48	&2	   &38 &68 &56	 &\textcolor{red}{\textbf{98}}\\
{\textit{faceocc2}}&23   &54	    &41	    &82	    &\textcolor{red}{\textbf{99}}
&48	    &94	    &79	    &94	    &87	    &79	    &88	  &95	  &56	&97	   &\textcolor{blue}{\textbf{98}} &79	 &54 &\textcolor{red}{\textbf{99}}\\
{\textit{girl}}&7   &70	    &49	    &91 	&64
&68	    &28	    &68	    &56	    &79	    &59	    &71	  &\textcolor{blue}{\textbf{97}}	  &74	&3	   &27 &43 &51	 &\textcolor{red}{\textbf{98}}\\
{\textit{jumping}} &2 &34 	&81	    &22	    &\textcolor{red}{\textbf{100}}
 &82	    &\textcolor{red}{\textbf{100}}	&\textcolor{blue}{\textbf{87}}	    &13	    &76	    &12	    &\textcolor{red}{\textbf{100}}  &18	 &\textcolor{red}{\textbf{100}}	&6 	   &\textcolor{red}{\textbf{100}}	 &\textcolor{red}{\textbf{100}} &5	&\textcolor{red}{\textbf{100}}\\
{\textit{mountainbike}}&14 &13	&82	  &71	&\textcolor{red}{\textbf{100}}
&\textcolor{blue}{\textbf{99}}	&18
&\textcolor{red}{\textbf{100}}	&61	    &26	    &35
&\textcolor{red}{\textbf{100}}  &98	  &25	&55	   &89
&\textcolor{red}{\textbf{100}} &74
&\textcolor{red}{\textbf{100}}\\
%MH2: what is skee? it shoudl be ski, right? I change skee to ski
%KH2: Yes, it it.
{\textit{ski}}&22 &5	&65	&55	&16	&58	&33	&6	&5	&36	&6	&9	&\textcolor{red}{\textbf{76}}	 &43	&1	 &60	 &1 &\textcolor{blue}{\textbf{71}}	 &60\\
{\textit{shaking}}&2  &25	&30	&14	&1
&39	&83	    &\textcolor{red}{\textbf{98}}	    &3	    &15	    &84	    &2	  &48	  &12	&4	   &84 &36	 &48 &\textcolor{blue}{\textbf{96}}\\
{\textit{skating}}&4   &11	&39	&22	&8
&68	&21	    &\textcolor{red}{\textbf{98}}	    &65	    &37	    &19	    &10	  &84	  &30	&14	   &89 &\textcolor{blue}{\textbf{94}} &69	&90\\
{\textit{sylvester}}&70 &34	&67	&61	&45
&66	&77	    &33	    &40	    &\textcolor{red}{\textbf{89}}	    &33	    &68	  &81	  &84	&6	   &77 &84 &\textcolor{blue}{\textbf{85}}	&74\\
{\textit{trellis}}&54    &39	&27	&36	&25
&12	&44	    &50	    &46	    &60	    &51	    &38	  &73	  &79	&40	   &44 &82 &\textcolor{blue}{\textbf{86}}	 &\textcolor{red}{\textbf{90}}\\
{\textit{woman}}&52      &27	&30	    &16	  &21
&18	&21	    &35	    &8	    &31	    &93	    &19	  &\textcolor{red}{\textbf{96}}	  &28	&2	   &19 &21 &66	 &\textcolor{blue}{\textbf{94}}\\\hline

Average SR  &34 &36     &42     &40     &50    &46    &54    &53    &46    &62    &54    &56    &\textcolor{blue}{\textbf{76}}    &60 &32    &65    &65 &66   &\textcolor{red}{\textbf{90}}\\\hline

\end{tabular}

}
\end{table*}
%-------------------------------------------------------------------

\begin{table*}[!ht]
\caption{Center location error (CLE)(in pixels) and
    average frame per second (FPS). \textcolor{red}{\textbf{Red}}
    fonts indicate the
    best performance while the \textcolor{blue}{\textbf{blue}} fonts
    indicate the second
    best ones. The total number of evaluated frames is $9,607$.
%    (Sort in alphabetical order).
}
\label{Table2}
{%\footnotesize
\scriptsize
%\center\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}|c|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|}\hline
\begin{center}
\center\begin{tabular}{| c |c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\hline
Sequence          &SMS &Frag &SSB &LOT &IVT &OAB  &MIL &VTD &L1T  &TLD  &DF &MTT &Struck &ConT &MOS &CT &CST &LGT &\textbf{STC}    \\ \hline

{\textit{animal}} &78   &100	  &25	&70	 &146
&62  &32     &17     &122    &125    &252    &17   &19      &76  &281   &18 &\textcolor{blue}{\textbf{16}}  &166 &\textcolor{red}{\textbf{15}}\\
{\textit{bird}}&25     &13	&101	&99	&13	 &\textcolor{red}{\textbf{9}}	&140	&57	&60	&145	 &12	 &156	 &21	&139	 &159	&79	&20	 &\textcolor{blue}{\textbf{11}} &15\\
{\textit{bolt}}&42     &43	&102	&\textcolor{blue}{\textbf{9}}	&65
 &227	    &\textcolor{blue}{\textbf{9}}	    &177	&261	&286	&277	&293  &149	   &126	&223   &10 &210  &12 &\textcolor{red}{\textbf{8}} \\
{\textit{cliffbar}}&41  &34	&56	&36	&37
&33	    &13	    &30	    &40	    &70	    &52	    &25	  &46	   &49	&104   &\textcolor{blue}{\textbf{6}}  &\textcolor{blue}{\textbf{6}}	 &10 &\textcolor{red}{\textbf{4}}\\
{\textit{chasing}}&13  &9	&44	&32	&6
 &9	    &13	    &23	    &9	    &47	    &31	    &\textcolor{blue}{\textbf{5}}	  &6	   &16	&68    &10 &\textcolor{blue}{\textbf{5}} &6	 &\textcolor{red}{\textbf{4}}\\
\textit{car4} &144    &56	&104	&177	&14
&109	    &63	    &127	&16	    &13	    &92	    &158  &\textcolor{red}{\textbf{9}}	   &11	&117   &63 &44	 &47 &\textcolor{blue}{\textbf{10}}\\
{\textit{car11}}&86  &117	&11	&30	&\textcolor{blue}{\textbf{7}}
 &11	    &8	    &20	    &8	    &12	    &\textcolor{red}{\textbf{6}}	    &8	  &9	   &8 	&8	   &9  &8 &16	 &\textcolor{blue}{\textbf{7}}\\
{\textit{cokecan}}&60   &70	&15	&46	&64
&11	    &18	    &68	    &40	    &29	    &30	    &10	  &\textcolor{blue}{\textbf{7}}	   &36	&53	   &16 &9	 &32 &\textcolor{red}{\textbf{6}}\\
{\textit{downhill}}&14  &11	&102	&226	&22	    &12	  &117	&\textcolor{blue}{\textbf{9}}	 &35	 &255	 &10	   &77	 &10	   &62	&116	 &12	 &129	&12 &\textcolor{red}{\textbf{8}}\\
{\textit{dollar}}&55  &56	&66	&66	&23
&28	    &23	    &65	    &65	    &72	    &\textcolor{blue}{\textbf{3}}	    &71	  &18	   &5	&12	   &20 &5	 &4 &\textcolor{red}{\textbf{2}}\\
{\textit{davidindoor}}&176 &103	&45	&100	&281
&43	    &33	    &40	    &86	    &13	    &27	    &\textcolor{blue}{\textbf{11}}	  &20	   &22	&78	   &28 &149 &12	 &\textcolor{red}{\textbf{6}}\\
{\textit{davidoutdoor}}&15 &60	&246	&\textcolor{blue}{\textbf{13}}	&75
&204	&70	    &67	    &88	    &263	&51	    &339  &107	   &102	&382   &107&129 &69	 &\textcolor{red}{\textbf{7}}\\
{\textit{faceocc2}}&50   &57	&39	&17	&\textcolor{red}{\textbf{9}}
&36	    &16	    &36	    &18	    &24	    &22	    &19	  &15	   &34	&\textcolor{blue}{\textbf{11}} &12 &23 &36	 &15\\
{\textit{girl}}&130   &26	&50	&\textcolor{blue}{\textbf{12}}	&36
&22	    &34	    &41	    &51	    &23	    &27	    &23	  &\textcolor{red}{\textbf{8}}	   &34	&126   &39 &43	 &35 &\textcolor{blue}{\textbf{12}}\\
{\textit{jumping}}&63 &30	&11	&43	&\textcolor{blue}{\textbf{4}}
&11	    &\textcolor{blue}{\textbf{4}}	    &17	    &45	    &13	    &73	    &7	  &42	  &\textcolor{blue}{\textbf{4}}	 &155	&6	 &\textcolor{red}{\textbf{3}}	&89  &\textcolor{blue}{\textbf{4}}\\
{\textit{mountainbike}}&135   &209	&11	    &24	    &\textcolor{red}{\textbf{5}}
&11	&208	&7	    &74	    &213	&155	&7	  &8	   &149	&16	   &11 &\textcolor{red}{\textbf{5}} &12	 &\textcolor{blue}{\textbf{6}}\\
{\textit{ski}}&91  &134	&\textcolor{blue}{\textbf{10}}	&12	&51	&11	&15	&179	&161	&222	&147	&33	 &\textcolor{red}{\textbf{8}}	&78	&386	 &11	&237	&13 &12\\
{\textit{shaking}}&224    &55	    &133	&90	    &134
&22	&11	    &\textcolor{red}{\textbf{5}}	    &72	    &232	&11	    &115  &23	   &191	&194   &11 &21	 &33 &\textcolor{blue}{\textbf{10}}\\
{\textit{skating}}&225   &176	    &76	    &102	&144
&74	&136	&\textcolor{red}{\textbf{9}}	    &87	    &204	&174	&78	  &16	   &151	&95	   &15 &\textcolor{blue}{\textbf{10}} &19	&15\\
{\textit{sylvester}}&15   &47	&14	    &23	    &138
&12	&9	    &66	    &49	    &\textcolor{blue}{\textbf{8}}	    &56	    &18	  &9	   &13	&65	   &9  &\textcolor{red}{\textbf{7}}	&11 &11\\
{\textit{trellis}}&32    &62    &65	    &48	    &153
&100	&43	    &32	    &44	    &70	    &48	    &80	  &25	   &19	&69	   &42 &35 &\textcolor{red}{\textbf{15}}	 &\textcolor{blue}{\textbf{17}}\\
{\textit{woman}}&49  &118	    &86	    &131	&112
&120	&119	&110	&148	&108	&12	    &169  &\textcolor{red}{\textbf{4}}	   &55	&176   &122&160	 &23 &\textcolor{blue}{\textbf{11}}\\\hline
Average CLE    &78 &67     &59     &64     &84     &51    &43    &53    &60    &78    &55    &66    &\textcolor{blue}{\textbf{21}}    &46 &100    &29    &51  &24  &\textcolor{red}{\textbf{10}} \\\hline
Average FPS   &12 &7 &11 &0.7
&33 &22 &38 &5 &1 &28 &13 &1 &20 &15  &\textcolor{blue}{\textbf{200}} &90 &120 &8    &\textcolor{red}{\textbf{350}} \\ \hline
\end{tabular}
\end{center}
}
\end{table*}
%

%
\begin{figure*}[!ht]
\begin{center}
\begin{tabular}{c}

\includegraphics[width=.29\linewidth]{ffigs/shaking/35}
\includegraphics[width=.29\linewidth]{ffigs/shaking/65}
\includegraphics[width=.29\linewidth]{ffigs/shaking/150}\\
\scriptsize (a) shaking \\
\includegraphics[width=.29\linewidth]{ffigs/woman/130}
\includegraphics[width=.29\linewidth]{ffigs/woman/150}
\includegraphics[width=.29\linewidth]{ffigs/woman/230}\\
\scriptsize (b) woman\\
\includegraphics[width=.29\linewidth]{ffigs/girl/100}
\includegraphics[width=.29\linewidth]{ffigs/girl/120}
\includegraphics[width=.29\linewidth]{ffigs/girl/440}\\
\scriptsize (c) girl\\
\includegraphics[width=.29\linewidth]{ffigs/chasing/390}
\includegraphics[width=.29\linewidth]{ffigs/chasing/470}
\includegraphics[width=.29\linewidth]{ffigs/chasing/520}\\
\scriptsize (d) chasing\\
\includegraphics[width=.29\linewidth]{ffigs/sylv/10}
\includegraphics[width=.29\linewidth]{ffigs/sylv/470}
\includegraphics[width=.29\linewidth]{ffigs/sylv/1000}\\
\scriptsize (e) sylvester\\
\end{tabular}
\includegraphics[width=0.7\linewidth]{ffigs/legend}
\end{center}
\caption{Tracking results of evaluated algorithms.}
\label{fig:screenshots}
\end{figure*}
%
\begin{figure*}[t]
\begin{center}
\begin{tabular}{c}
\includegraphics[width=.29\linewidth]{ffigs/dollar/10}
\includegraphics[width=.29\linewidth]{ffigs/dollar/100}
\includegraphics[width=.29\linewidth]{ffigs/dollar/300}\\
\scriptsize (a) dollar \\
\includegraphics[width=.29\linewidth]{ffigs/downhill/2}
\includegraphics[width=.29\linewidth]{ffigs/downhill/100}
\includegraphics[width=.29\linewidth]{ffigs/downhill/340}\\
\scriptsize (b) downhill\\
\includegraphics[width=.29\linewidth]{ffigs/car4/2}
\includegraphics[width=.29\linewidth]{ffigs/car4/220}
\includegraphics[width=.29\linewidth]{ffigs/car4/580}\\
\scriptsize (c) car4\\
\end{tabular}
\includegraphics[width=0.7\linewidth]{ffigs/legend}
\end{center}
\caption{Tracking results of evaluated algorithms.}
\label{fig:screenshots1}
\end{figure*}
%
\subsection{Experimental Results}
We use two evaluation criteria to quantitatively evaluate the $19$
trackers: the center location error (CLE) and success rate (SR), both
computed based on the manually labeled ground truth results of each frame.
%
The score of success rate is defined as
$score=\frac{area(R_t\bigcap R_g)}{area(R_t\bigcup R_g)}$,
where $R_t$ is a tracked bounding box and $R_g$ is the ground
truth bounding box, and the result of one frame
is considered as a success if $score>0.5$.
%
Table~\ref{Table1} and Table~\ref{Table2}
show the quantitative results in which the proposed STC tracker
achieves the best or second best performance in most sequences both in
terms of center location error and success rate.
%
%MH: on average not in average
Furthermore, the proposed tracker is the most efficient ($350$ FPS on
average) algorithm among all evaluated methods.
%
Although the CST~\cite{henriques2012circulant} and
MOS~\cite{bolme2010visual} methods also use FFT for fast computation,
the CST method performs time-consuming kernel operations
and the MOS tracker computes several correlation filters in
each frame, thereby making these two approaches less efficient than the
proposed algorithm.
%
Figures~\ref{fig:screenshots},~\ref{fig:screenshots1},~\ref{fig:screenshots2},~\ref{fig:cliffbar} show some tracking results of different
trackers.
%MH2: revised
For presentation clarity, we only show the results of the top $7$
trackers in terms of average success rates.

%
\begin{figure*}[ht]
\begin{center}
\begin{tabular}{c}
\includegraphics[width=.29\linewidth]{ffigs/cokecan/50}
\includegraphics[width=.29\linewidth]{ffigs/cokecan/175}
\includegraphics[width=.29\linewidth]{ffigs/cokecan/255}\\
\scriptsize (a) cokecan \\
\includegraphics[width=.29\linewidth]{ffigs/cliffbar/150}
\includegraphics[width=.29\linewidth]{ffigs/cliffbar/200}
\includegraphics[width=.29\linewidth]{ffigs/cliffbar/300}\\
\scriptsize (b) cliffbar\\
\includegraphics[width=.29\linewidth]{ffigs/bird/16}
\includegraphics[width=.29\linewidth]{ffigs/bird/50}
\includegraphics[width=.29\linewidth]{ffigs/bird/95}\\
\scriptsize (c) bird\\
\includegraphics[width=.29\linewidth]{ffigs/trellis/210}
\includegraphics[width=.29\linewidth]{ffigs/trellis/500}
\includegraphics[width=.29\linewidth]{ffigs/trellis/560}\\
\scriptsize (d) trellis\\
\includegraphics[width=.29\linewidth]{ffigs/car11/2}
\includegraphics[width=.29\linewidth]{ffigs/car11/180}
\includegraphics[width=.29\linewidth]{ffigs/car11/280}\\
\scriptsize (e) car11\\
\end{tabular}
\includegraphics[width=0.7\linewidth]{ffigs/legend}
\end{center}
\caption{Tracking results of evaluated algorithms.}
\label{fig:cliffbar}
\end{figure*}
%\vspace{-3mm}
{\flushleft\textbf{Illumination, scale and pose variation.}}
There are large illumination variations in the evaluated sequences.
%
%MH: bad sentnece, the sequene does not changes apperance, but the
%apperance of the car changes
%The~\textit{car4} sequence changes appearance drastically
%due to the cast shadows and ambient lights.
%
The appearance of the target object in the~{\textit{car4}} sequence
changes significantly due to the cast shadows and ambient lights.
%
Only the models of the IVT, L1T, Struck and STC methods adapt to these
illumination variations well.
%
Likewise, only the VTD and our STC methods perform favorably
on the~{\textit{shaking}} sequence because the object appearance
changes drastically due to the stage lights and sudden pose variations
($\#65, \#150$ in the~{\textit{shaking}} sequence shown in
Figure~\ref{fig:screenshots}).
%
The~{\textit{davidindoor}}, {\textit{trellis}} and {\textit{skating}}
sequences contain gradual pose and scale variations as well as
illumination changes.
%
Note that most reported results using the~{\textit{davidindoor}} and
{\textit{trellis}} sequences are only on subsets of the available
frames, i.e., not from the very beginning of the~{\textit{davidindoor}}
video when the face is in complete dark, or until the very end of
the~{\textit{trellis}} clip when the appearance undergoes both
sudden pose and illumination variations.
%
In this work, the full sequences are used to better evaluate the
performance of all algorithms.
%
Only the proposed algorithm is able to achieve favorable
tracking results on these three sequences both in terms of accuracy and
success rate.
%
This can be attributed to the use of spatio-temporal
context information which facilitates filtering out noisy observations
(as discussed in Section~\ref{discussion}),
thereby enabling the proposed STC algorithm to relocate
the target when object appearance changes drastically due to
illumination, scale and pose variations.
%
{\flushleft\textbf{Occlusion, rotation, and pose variation.}}
The target object in the~{\textit{davidoutdoor}} sequence is heavily
occluded by the tree in some frames (See $\#84, \#188$ of
the {\textit{davidoutdoor}} sequence in
Figure~\ref{fig:challengingEnvironments}) while the target
objects in the~{\textit{woman}}, {\textit{girl}}, {\textit{bird}} and
{\textit{faceocc2}} sequences are partially occluded at times.
%
The objects in the~{\textit{davidoutdoor}} and {\textit{girl}} sequences
also undergo in-plane rotation (See the~{\textit{davidoutdoor}} sequence in
Figure~\ref{fig:challengingEnvironments} and $\#100, \#120$ of
the {\textit{girl}} sequence in Figure~\ref{fig:screenshots}) which makes
the tracking tasks difficult.
%
Only the proposed algorithm
is able to track the objects successfully in most frames of these two
sequences.
%
The~{\textit{woman}} sequence has non-rigid deformation and
heavy occlusion (See $\#130,\#150, \#230$ of the~{\textit{woman}} sequence
in Figure~\ref{fig:screenshots}) at the same time.
%
All the other
trackers fail to successfully track the object except the Struck
and the proposed STC algorithms.
%
As most of the local contexts surrounding the target objects are not
occluded in these sequences, such information facilitates the proposed algorithm
relocating the object even they are almost fully occluded.
%
\begin{figure*}[!ht]
\begin{center}
\begin{tabular}{c}
\includegraphics[width=.29\linewidth]{ffigs/faceocc2/100}
\includegraphics[width=.29\linewidth]{ffigs/faceocc2/260}
\includegraphics[width=.29\linewidth]{ffigs/faceocc2/760}\\
\scriptsize (a) faceocc2 \\
\includegraphics[width=.29\linewidth]{ffigs/jumping/2}
\includegraphics[width=.29\linewidth]{ffigs/jumping/200}
\includegraphics[width=.29\linewidth]{ffigs/jumping/300}\\
\scriptsize (b) jumping\\
\includegraphics[width=.29\linewidth]{ffigs/mountainbike/2}
\includegraphics[width=.29\linewidth]{ffigs/mountainbike/100}
\includegraphics[width=.29\linewidth]{ffigs/mountainbike/210}\\
\scriptsize (c) mountainbike\\
\includegraphics[width=.29\linewidth]{ffigs/skating/30}
\includegraphics[width=.29\linewidth]{ffigs/skating/180}
\includegraphics[width=.29\linewidth]{ffigs/skating/380}\\
\scriptsize (d) skating\\
\includegraphics[width=.29\linewidth]{ffigs/ski/10}
\includegraphics[width=.29\linewidth]{ffigs/ski/30}
\includegraphics[width=.29\linewidth]{ffigs/ski/50}\\
\scriptsize (e) ski\\
\end{tabular}
\includegraphics[width=0.7\linewidth]{ffigs/legend}
\end{center}
\caption{Tracking results of evaluated algorithms.}
\label{fig:screenshots2}
\end{figure*}
%
{\flushleft\textbf{Background clutter and abrupt motion.}}
In the~{\textit{animal}}, {\textit{cokecan}} and {\textit{cliffbar}}
sequences, the target objects undergo fast movements in the cluttered
backgrounds.
%
The target object in the~{\textit{chasing}} sequence undergoes abrupt motion
with 360 degree out-of-plane rotation, and the proposed algorithm
achieves the best performance both in terms of success rate and center
location error.
%
The~{\textit{cokecan}} video contains a specular object with
in-plane rotation and heavy occlusion, which makes this tracking task
difficult.
%
Only the Struck and the proposed STC methods are able to
successfully track most of the frames.
%
In the {\textit{cliffbar}} sequence, the texture in the background is
very similar to that of the target object.
%
Most trackers drift to background except the CT, CST, LGT and
our methods (See $\#300$ of the {\textit{cliffbar}} sequence in
Figure~\ref{fig:cliffbar}).
%
Although the target and its local background have very similar
texture, their spatial relationships are different which are used by
the proposed algorithm when learning a spatial context model.
%
Hence, the proposed STC algorithm is able to separate the target object
from the background based on the spatio-temporal context.
%
\section{Analysis of CT and STC}
Both CT and STC are based on the tracking by detection framework. However, the CT explores the local Haar-like features and the naive Bayes classifier while the STC exploits the global image intensity feature and the spatio-temporal context filter.


%
The comparison results of CT and STC in terms of success rate and center location error shown by Table~\ref{Table1} and Table~\ref{Table2} demonstrate that the STC performs much better than the CT. Furthermore, the STC runs much faster than the CT method, which validates the effectiveness of context learning.
\subsection{Discussion}
The potential applications of CT are very wide that include the face verification and image classification, etc. The STC method works very fast and robust that is well applied to visual tracking. How about fusing these two methods to a framework? In the next section, we will demonstrate the fusion results.

\subsection{Fusion of CT and STC}
We adopt a simple fusion strategy as illustrated by Figure~\ref{fig:coarsefine}: first, we use the STC to find the coarse location, and then based on the coarse location, we use the CT to find the fine location. Finally, we linearly combine the coarse and fine locations to achieve the final location. Table~\ref{table:sr} demonstrates the results in terms of success rate for CT, STC and fusion method. We can observe that the fusion method achieves the best or second best results for most sequence.
%
\begin{table}[t]
\caption{Success rate (SR)($\%$).
   The \textcolor{red}{\textbf{red}} fonts indicate the best performance while the \textcolor{blue}{\emph{italic}} fonts indicate the second     best ones.}
\label{table:sr}
{\center\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}|r||c|c|c|}\hline
Sequence         &CT    &STC  &Fusion   \\ \hline\hline
\textit{animal}        &92     &\textcolor{blue}{\textit{94}}    &\textcolor{red}{\textbf{96}}   \\ \hline
\textit{bird}          &8      &\textcolor{blue}{\textit{63}}    &\textcolor{red}{\textbf{75}}                   \\ \hline
\textit{bolt}          &94     &\textcolor{red}{\textbf{96}}                               &\textcolor{blue}{\textit{95}}                    \\ \hline
\textit{cliffbar}      &95     &\textcolor{red}{\textit{97}}                               &\textcolor{red}{\textbf{98}}                    \\ \hline
\textit{chasing}       &\textcolor{blue}{\textit{79}}     &\textcolor{red}{\textbf{98}}                               &\textcolor{red}{\textbf{98}}                    \\ \hline
\textit{car4}          &36     &\textcolor{red}{\textit{98}}                               &\textcolor{red}{\textbf{99}}                    \\ \hline
\textit{car11}         &36     &\textcolor{blue}{\textit{63}}                               &\textcolor{red}{\textbf{78}}                    \\ \hline
\textit{cokecan}       &30     &\textcolor{blue}{\textit{92}}                               &\textcolor{red}{\textbf{95}}                    \\ \hline
Mean          &59     &\textcolor{blue}{\textit{88}}                               &\textcolor{red}{\textbf{92}}                    \\ \hline
\end{tabular*}
}
\end{table}
%
\begin{figure}[!ht]
\begin{center}
\begin{tabular}{c}
\includegraphics[width=0.7\linewidth]{images/coarsefine}
\end{tabular}
\end{center}
\caption{Illustration of fusion strategy.}
\label{fig:coarsefine}
\end{figure}
%
\section{Summary}
In this chapter, we presented a simple yet fast and robust
algorithm which exploits spatio-temporal context information for
visual tracking.
%
Two local context models (i.e., spatial context
and spatio-temporal context models) were proposed which are
robust to appearance variations introduced by occlusion, illumination
changes, and pose variations.
%
The Fast Fourier Transform algorithm is
used in both online learning and detection, thereby resulting in
an efficient tracking method that runs at $350$ frames per
second with MATLAB implementation.
%
Numerous experiments with state-of-the-art algorithms on
challenging sequences demonstrate that the proposed algorithm achieves
favorable results in terms of accuracy, robustness, and speed.
% 